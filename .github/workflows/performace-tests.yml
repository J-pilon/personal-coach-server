name: Performance Tests

on: 
  pull_request:
    branches: [ main ]

permissions:
  contents: read
  pull-requests: write

jobs:
  performance-test:
    runs-on: ubuntu-latest
    outputs:
      formatted_results: ${{ steps.format-results.outputs.results }}

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: personal_coach_test
        options: >-
          --health-cmd pg_isready
          --health-interval 7s
          --health-timeout 3s
          --health-retries 3
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 7s
          --health-timeout 3s
          --health-retries 3
        ports:
          - 6379:6379

    env: 
      RAILS_MASTER_KEY: ${{ secrets.RAILS_MASTER_KEY }}
      RAILS_ENV: test
      DATABASE_URL: postgres://postgres:postgres@localhost:5432/personal_coach_test
      PGHOST: localhost
      PGUSER: postgres
      PGPASSWORD: postgres

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.3.0'
          bundler-cache: true

      - name: Setup Node.js (for K6)
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Setup K6
        uses: grafana/setup-k6-action@v1

      - name: Set up test credentials
        run: |
          mkdir -p config/credentials
          echo "${{ secrets.RAILS_TEST_KEY }}" > config/credentials/test.key

      - name: Create and seed database
        run: |
          bundle exec rails db:create
          bundle exec rails db:migrate
          bundle exec rails db:seed -- --user-count=50

      - name: Start Rails server
        env:
          RAILS_LOG_TO_STDOUT: true
        run: |
          bundle exec rails s -p 3000 &
          sleep 5
          # Wait for server to be ready
          until curl -f http://localhost:3000/up; do
            sleep 1
          done

      - name: Run performance tests
        run: |
          chmod +x scripts/run-all-performance-tests.sh
          ./scripts/run-all-performance-tests.sh
      
      - name: Format test results
        id: format-results
        run: |
          # Parse aggregated JSON and format as markdown
          AGGREGATED_JSON="performance-tests/results/aggregated.json"
          
          if [ ! -f "$AGGREGATED_JSON" ]; then
            echo "‚ùå Aggregated results not found"
            exit 1
          fi
          
          # Start building markdown output
          OUTPUT="## üöÄ Performance Test Results"
          OUTPUT+="\n"
          
          # Get all test names
          TEST_NAMES=$(jq -r 'keys[]' "$AGGREGATED_JSON")
          
          for test_name in $TEST_NAMES; do
            test_display_name=$(echo "${test_name//_/ }" | tr '[:lower:]' '[:upper:]' | sed 's/\([A-Z]\)/\L\1/g' | sed 's/^\(.\)/\U\1/g' | sed 's/ \([a-z]\)/\ \U\1/g')
            OUTPUT+="\n### ${test_display_name}"
            OUTPUT+="\n"
            
            # Extract metrics for this test (K6 summary JSON structure)
            P95=$(jq -r ".\"${test_name}\".metrics.http_req_duration.\"p(95)\" // 0" "$AGGREGATED_JSON")
            P90=$(jq -r ".\"${test_name}\".metrics.http_req_duration.\"p(90)\" // 0" "$AGGREGATED_JSON")
            AVG=$(jq -r ".\"${test_name}\".metrics.http_req_duration.avg // 0" "$AGGREGATED_JSON")
            FAIL_RATE=$(jq -r ".\"${test_name}\".metrics.http_req_failed.value // 0" "$AGGREGATED_JSON")
            
            # Extract check info (with defaults for null)
            CHECKS_PASSED=$(jq -r ".\"${test_name}\".metrics.checks.passes // 0" "$AGGREGATED_JSON")
            CHECKS_FAILED=$(jq -r ".\"${test_name}\".metrics.checks.fails // 0" "$AGGREGATED_JSON")
            TOTAL_REQUESTS=$(jq -r ".\"${test_name}\".metrics.http_reqs.count // 0" "$AGGREGATED_JSON")
            
            # Handle null values
            [ "$CHECKS_PASSED" = "null" ] && CHECKS_PASSED="0"
            [ "$CHECKS_FAILED" = "null" ] && CHECKS_FAILED="0"
            [ "$TOTAL_REQUESTS" = "null" ] && TOTAL_REQUESTS="0"
            
            # Manually evaluate thresholds by comparing actual values
            # Extract threshold definitions (e.g., "p(95)<500" or "rate<0.001")
            P95_THRESHOLD_DEF=$(jq -r ".\"${test_name}\".metrics.http_req_duration.thresholds | keys[0] // \"\"" "$AGGREGATED_JSON")
            FAIL_THRESHOLD_DEF=$(jq -r ".\"${test_name}\".metrics.http_req_failed.thresholds | keys[0] // \"\"" "$AGGREGATED_JSON")
            
            # Default to pass
            P95_THRESHOLD="true"
            FAIL_THRESHOLD="true"
            
            # Check http_req_duration threshold (extract number and compare)
            if [ -n "$P95_THRESHOLD_DEF" ] && [ "$P95" != "N/A" ]; then
              THRESHOLD_VAL=$(echo "$P95_THRESHOLD_DEF" | grep -oP '<\K[0-9.]+' || echo "")
              if [ -n "$THRESHOLD_VAL" ]; then
                # Use bc for float comparison
                if [ "$(echo "$P95 >= $THRESHOLD_VAL" | bc -l 2>/dev/null || echo 0)" = "1" ]; then
                  P95_THRESHOLD="false"
                fi
              fi
            fi
            
            # Check http_req_failed threshold (extract number and compare)
            if [ -n "$FAIL_THRESHOLD_DEF" ] && [ "$FAIL_RATE" != "null" ]; then
              FAIL_THRESHOLD_VAL=$(echo "$FAIL_THRESHOLD_DEF" | grep -oP '<\K[0-9.]+' || echo "")
              if [ -n "$FAIL_THRESHOLD_VAL" ]; then
                # Use bc for float comparison
                if [ "$(echo "$FAIL_RATE >= $FAIL_THRESHOLD_VAL" | bc -l 2>/dev/null || echo 0)" = "1" ]; then
                  FAIL_THRESHOLD="false"
                fi
              fi
            fi
            
            # Format numbers (handle null/empty values)
            if [ "$P95" != "null" ] && [ -n "$P95" ]; then
              P95=$(printf "%.2f" "$P95" 2>/dev/null || echo "N/A")
            else
              P95="N/A"
            fi
            
            if [ "$P90" != "null" ] && [ -n "$P90" ]; then
              P90=$(printf "%.2f" "$P90" 2>/dev/null || echo "N/A")
            else
              P90="N/A"
            fi
            
            if [ "$AVG" != "null" ] && [ -n "$AVG" ]; then
              AVG=$(printf "%.2f" "$AVG" 2>/dev/null || echo "N/A")
            else
              AVG="N/A"
            fi
            
            # FAIL_RATE from K6 summary is 0-1 range, multiply by 100 for percentage
            if [ "$FAIL_RATE" != "null" ] && [ -n "$FAIL_RATE" ]; then
              FAIL_RATE_PCT=$(echo "scale=2; $FAIL_RATE * 100" | bc -l 2>/dev/null || echo "0.00")
            else
              FAIL_RATE_PCT="N/A"
            fi
            
            # Determine status icons
            P95_STATUS="‚úÖ"
            [ "$P95_THRESHOLD" = "false" ] && P95_STATUS="‚ùå"
            
            FAIL_STATUS="‚úÖ"
            [ "$FAIL_THRESHOLD" = "false" ] && FAIL_STATUS="‚ùå"
            
            # Build table
            OUTPUT+="\n| Metric | Value | Status |"
            OUTPUT+="\n|--------|-------|--------|"
            OUTPUT+="\n| **95th Percentile Latency** | ${P95}ms | ${P95_STATUS} |"
            OUTPUT+="\n| **90th Percentile Latency** | ${P90}ms | - |"
            OUTPUT+="\n| **Average Latency** | ${AVG}ms | - |"
            OUTPUT+="\n| **Failure Rate** | ${FAIL_RATE_PCT}% | ${FAIL_STATUS} |"
            OUTPUT+="\n"
            OUTPUT+="\n**Summary:** ${TOTAL_REQUESTS} requests, ${CHECKS_PASSED} checks passed, ${CHECKS_FAILED} checks failed"
            OUTPUT+="\n"
            OUTPUT+="\n---"
            OUTPUT+="\n"
          done
          
          # Save to output
          echo "results<<EOF" >> $GITHUB_OUTPUT
          echo -e "$OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Formatted results for $(echo "$TEST_NAMES" | wc -l) test(s)"

  update-pr-with-results:
    needs: [performance-test]
    uses: ./.github/workflows/reusable-update-pr-description.yml
    with:
      content: ${{ needs.performance-test.outputs.formatted_results }}
      section_id: PERF_RESULTS
      body_update_action: section
